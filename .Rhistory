knitr::opts_chunk$set(echo = TRUE)
if(!"tinytex" %in% installed.packages()) { tinytex::install_tinytex()} #installing tinytex
# List of packages for the project
packages = c("tidyverse",
"RWeka",
"Hmisc",
"caret",
"tidyr",
"purrr",
"party",
"partykit",
"gam",
"MLmetrics",
"e1071",
"Rborist",
"randomForest",
"LiblineaR",
"lubridate",
"knitr",
"ggcorrplot",
"gbm",
"xfun",
"reactable",
"kableExtra",
"lemon"
)
# Install CRAN packages (if not already installed)
inst <- packages %in% installed.packages()
if(length(packages[!inst]) > 0) install.packages(packages[!inst], repos = "http://cran.us.r-project.org")
library(tidyverse)
library(RWeka)
library(Hmisc)
library(caret)
library(tidyr)
library(purrr)
library(party)
library(partykit)
library(gam)
library(e1071)
library(Rborist)
library(randomForest)
library(LiblineaR)
library(lubridate)
library(knitr)
library(ggcorrplot)
library(gbm)
library(xfun)
library(reactable)
library(kableExtra)
library(tinytex)
library(MLmetrics)
library(lemon)
#File URL
url <-
"https://archive.ics.uci.edu/ml/machine-learning-databases/00327/Training%20Dataset.arff"
#Creating a temporary file name
temp <- tempfile()
#Downloading the file.
download.file(url,temp)
#Reading the file into an R object
phishing <- read.arff(temp)
#remove the temporary file
unlink(temp)
#Calculating the pearson correlation coefficient and associated p value for each variable with the Results
correlation_matrix <- rcorr(as.matrix(phishing), type="pearson")
r <- correlation_matrix$r
p <- correlation_matrix$P
#Making data frame of correlation coefficient and p values for Results with other attributes/predictors.
rp <- cbind(r, p)
rp <- rp[,c(31, 62)] # Selecting only columns for correlation with Result
colnames(rp) <- c("r", "p") #Naming the columns
rp <- as.data.frame(rp) %>% mutate(significant=(p<=0.05), strong_r=(r>=0.2))
attribute <- colnames(phishing)
rp <- cbind(attribute, rp) #Essentially, adding a column to indicate the predictor.
rp <- rp[-c(31),] #Removing Result row from the data frame as we do not want to see the correlation of Result with result itself
#Plotting a scatter plot of the attributes (predictor) with their correlation coefficient with Result.
#The color is used to indicate if the given attribute has a strong correlation (r>=0.2) with Result.
#The shape is used to indicate if the p value of the correlation coefficient of the attributes are significant (p=<0.05).
plot1<-rp %>% ggplot(aes(x=attribute, y=r, shape=significant, col=strong_r)) +
geom_point(size=5) +
theme(axis.text.x = element_text(angle = 50, hjust = 1),
plot.title = element_text(hjust = 0.5)) +
scale_color_brewer(palette = "Dark2") +
labs(title="Correlation Plot of Features with Result",
x="Features",
y="Correlation Coeffecient",
col="Strong Correlation?",
shape="Significant p-value?")
plot1
#Choosing the 8 attributes (to be used as predictors) that have satisfied the above criteria.
to_choose<-(rp %>%
mutate(choose=(significant==TRUE & strong_r==TRUE)) %>%
filter(choose==TRUE))$attribute
#A final data set with those 8 attributes and Result is created and named 'phish'. This data set will be used for analysis and prediction.
phish<-phishing %>% select(to_choose, Result)
#Creating a data frame with the information about various attributes:
df <- data.frame(Attribute = c(colnames(phish)),
Value_Set = c("{-1,1}", "{-1, 0, 1}", "{-1, 0, 1}", "{-1, 1}", "{-1, 0, 1}",
"{-1, 0, 1}", "{-1, 0, 1}", "{-1, 0, 1}", "{-1, 1}"),
Explanation = c( "Hyphens are rarely used in legitimate URLs. Phishers tend to add prefixes or suffixes separated by hyphen from the domain name to trick users into assuming that they are dealing with a legitimate webpage. If the domain name part includes hyphen, the value is '-1' (indicating phishing). Otherwise, the value is '1' (indicating legitimate).",
"The number of dots in the URL is counted after omiting the 'www.' and the country-code top-level domain (ccTLD). If the number of dots so counted is 1, the attribute is given a value '1' (indicating legitimate). If the number of dots is 2, the value is '0' (indicating suspicious). If the number of dots is more than 2, the value is '-1' (indicating phishing).",
"SSL stands for Secure Sockets Layer. The certificate assigned with HTTPS is thououghly checked. The minimum age of a reputable certificate is two years. If the website uses https, the issuer is trusted and the age of certificate is more than 1 year, the value of the attribute is '1' (indicating legitimate). If the website uses https and the issuer is not trusted, the value is '0' (indicating suspicious). Otherwise, the value is '-1' (indicating phishing).",
"Request URL examines whether the external objects contained within a webpage such as images, videos and sounds are loaded from another domain. In legitimate webpages, the webpage address and most of objects embedded within the webpage are sharing the same domain. If request URLs loaded from other domains are less than 22%, the value of the attribute is '1' (indicating legitimate). If they are more than or equal to 22%, but less than 61%, the value is '0' (indicating suspicious). Otherwise, the value is '-1' (indicating phishing). In our data set, none of the websites are given the value 0. Hence, the possible values are only 1 and -1." ,
"An anchor is an element defined by the <a> tag. It is examined if the <a> tags and the website have different domain names and if the anchors link to any webpage. If such anchors are less than 31%, the attribute value is '1' (indicating legitimate). If they are more than or equal to 31%, but less than 67%, the value is '0' (indicating suspicious). Otherwise, the value is '-1' (indicating phishing).",
"It is common for legitimate websites to use <Meta> tags to offer metadata about the HTML document, <Script> tags to create a client side script, and <Link> tags to retrieve other web resources. If the links in such tags are less than 17%, the attribute value is '1' (indicating legitimate). If they are more than or equal to 17%, but less than 81%, the value is '0' (indicating suspicious). Otherwise, the value is '-1' (indicating phishing).",
"SFH stands for Server Form Handler.If the SFHs contain an empty string or 'about$:$blank' the attribute is assigned a value of '-1' (indicating phishing). If the domain name in SFHs are differnet from the domain name of the webpage, the value is '0' (indicating suspicious). Otherwise, the value is '1' (indicating legitimate).",
"Website traffic models the popularity of the website in terms of website rank (depending on the number of visitors and the number of pages they visit). If the website rank is less than 100,000, the attribute value is '1' (indicating legitimate). If the website rank is greater than 100,000, the value is '0' (indicating suspicious). Otherwise, (if the domain has no traffic or is not recognized by the database), the value is '-1' (indicating phishing).",
"Result is the outcome. If the website is actually legitimate, the value is '1'. Otherwise, it is given a value of '-1'."
))
#Using kbl() function to create a beautiful table.
library(kableExtra)
kbl(df, longtable=T, booktabs=T, caption="Attributes Information") %>%
column_spec(1, width="3.5cm") %>%
column_spec(2, width="2cm") %>%
column_spec(3, width="10cm") %>%
kable_styling(latex_options=c("repeat_header"))
#Using kbl to cretae a nice table showing the first 6 rows of phish:
library(kableExtra)
kbl(head(phish), longtable=T, booktabs=T, caption="First 6 Rows of the Data Set") %>%
column_spec(1:9, width="1.5cm") %>%
kable_styling(latex_options=c("repeat_header")) %>%
row_spec(0, angle= 45 )
#Creating a correlation matrix and converting it to a data frame:
phish_numeric_corr<-data.frame(apply(phish, 2, function(x) as.numeric(as.character(x)))) %>% select(-Result)
corr <- round(cor(phish_numeric_corr), 1)
p.mat <- cor_pmat(phish_numeric_corr)
#Making the correlation heat map:
ggcorrplot(corr, hc.order = TRUE, type = "lower",
lab = TRUE, p.mat = p.mat, legend.title = "Correlation") +
theme(plot.title = element_text(hjust = 0.5)) +
labs(title="Correlation Heatmap for Selected Features",
caption="X indicates that the correlation is insignificat (95% C.I.)") +
scale_color_brewer(palette = "Dark2")
#Finding the proportion of '-1's in the data set:
phish %>% summarise(pi = mean(Result == -1)) %>% pull(pi)
result_count <- phish %>% ggplot(aes(x=Result)) +
geom_bar(stat = "count") +
scale_color_brewer(palette = "Dark2") +
theme(plot.title = element_text(hjust = 0.5)) +
labs(title="Total Number of Phishing and Legitimate Websites in Dataset",
x="Website Type",
y="Count") +
scale_x_discrete(labels=c("Phishing", "Legitimate")) +
stat_count(geom = "text", colour = "white", size = 5,
aes(label = ..count..),position=position_stack(vjust=0.5))
result_count
phish2<-phish %>% mutate(Result=ifelse(Result==-1, "Phishing (-1)", "Legitimate (1)"))# Done just to change the text in legend.
all_variables<-gather(phish2, key="key", "value",-Result) %>%
ggplot(aes(x=value, fill=Result)) +
facet_wrap(~ key, scales = "free", ncol=3) +
geom_bar() +
scale_color_brewer(palette = "Dark2") +
theme(plot.title = element_text(hjust = 0.5, size = 9)) +
labs(title="Distribution of Results by Attributes",
x="Website Type Indicated\n-1: Phishing, 0: Suspicious, 1: Legitimate",
y="Count",
fill="Actual Result")
all_variables %>% reposition_legend('center', panel='panel-3-3')
#Setting the seed to 820. This will be done everywhere to have uniformity in prediction.
set.seed(820, sample.kind = "Rounding")
y <- phish$Result
#Creating the text index:
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
#Creating the test and training sets:
test_set <- phish[test_index, ]
train_set <- phish[-test_index, ]
phish_numeric <- data.frame(apply(phish, 2, function(x) as.numeric(as.character(x))))
#Spliting the numeric dat set:
set.seed(820, sample.kind = "Rounding")
y<-phish_numeric$Result
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
test_set <- phish_numeric[test_index, ]
train_set <- phish_numeric[-test_index, ]
lm_fit <- lm(Result ~ ., data = train_set)
p_hat <- predict(lm_fit, test_set)
#Defining d (the cut off value)
d<-seq(-1, 1, length=100)
#Making the required function which takes input as 'd' and returns the accuracy on the train set.
decision_function<-function(d){
p_hat <- predict(lm_fit, train_set)
y_hat <- ifelse(p_hat > d, 1, -1) %>% factor()
ytrain<-as.factor(train_set$Result)
confusionMatrix(y_hat, ytrain)$overall["Accuracy"]
}
#Applying the function to different values of d to make a data frame.
df<-data.frame(d=d, accuracy=sapply(d, decision_function))
#Plotting the data frame:
ggplot(df, aes(d, accuracy)) +
geom_line() +
labs(title="Variation of Accuracy with Value of d") +
geom_vline(xintercept = 0, col="red")+
theme(plot.title = element_text(hjust = 0.5),
axis.text.x = element_text(size=9),
axis.text.y = element_text(size=9))
p_hat <- predict(lm_fit, test_set)
d <- 0
y_hat <- ifelse(p_hat >= d, 1, -1) %>% factor()
ytest <- as.factor(test_set$Result)
confusionMatrix(y_hat, ytest)
#Accuracy on test set:
test_accuracy_lm<-confusionMatrix(y_hat, ytest)$overall["Accuracy"]
#F1-Score on test Set:
test_f1score_lm<-F1_Score(y_hat, ytest)
#Accuracy on train set:
p_hat <- predict(lm_fit, train_set)
d<-0
y_hat <- ifelse(p_hat >= d, 1, -1) %>% factor()
ytrain<-as.factor(train_set$Result)
train_accuracy_lm<-confusionMatrix(y_hat, ytrain)$overall["Accuracy"]
#Creating a vector of predictions:
predictions_lm<-c(train_accuracy_lm, test_accuracy_lm, test_f1score_lm)
#These vectors of prediction (consisting of accuracy on test set, F1 score on test set, and accuracy on train set) will be created for every algorithm. They wiull be compiled in a plot at the end (in the results section)
#Manipulating the data set to convert all values between '0' and '1'
phish_numeric <- data.frame(apply(phish, 2, function(x) as.numeric(as.character(x))))
phish_numeric[phish_numeric == "0"] <- 0.5
phish_numeric[phish_numeric == "-1"] <- 0
phish_numeric <- data.frame(apply(phish_numeric, 2, function(x) as.numeric(as.character(x))))
set.seed(820, sample.kind = "Rounding")
y<-phish_numeric$Result
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
test_set <- phish_numeric[test_index, ]
train_set <- phish_numeric[-test_index, ]
#Defining d (the cut-off value)
d<-seq(0, 1, length=100)
#Making the required function which takes input as 'd' and returns the accuracy on the train set.
decision_function<-function(d){
p_hat <- predict(glm_fit, train_set, type = "response")
y_hat <- ifelse(p_hat > d, 1, -1) %>% factor()
ytrain<-as.factor(phish[-test_index, ]$Result)
confusionMatrix(y_hat, ytrain)$overall["Accuracy"]
}
#Creating a data frame after applying the function:
df<-data.frame(d=d, accuracy=sapply(d, decision_function))
glm_fit <- glm(Result ~ ., data=train_set, family = "binomial")
p_hat <- predict(glm_fit, test_set)
#Defining d (the cut-off value)
d<-seq(0, 1, length=100)
#Making the required function which takes input as 'd' and returns the accuracy on the train set.
decision_function<-function(d){
p_hat <- predict(glm_fit, train_set, type = "response")
y_hat <- ifelse(p_hat > d, 1, -1) %>% factor()
ytrain<-as.factor(phish[-test_index, ]$Result)
confusionMatrix(y_hat, ytrain)$overall["Accuracy"]
}
#Creating a data frame after applying the function:
df<-data.frame(d=d, accuracy=sapply(d, decision_function))
#Generating the plot:
ggplot(df, aes(d, accuracy)) +
geom_line() +
labs(title="Variation of Accuracy with Value of d") +
geom_vline(xintercept = 0.5, col="red")+
theme(plot.title = element_text(hjust = 0.5),
axis.text.x = element_text(size=9),
axis.text.y = element_text(size=9))
p_hat <- predict(glm_fit, test_set)
d <- 0.5
y_hat <- ifelse(p_hat >= d, 1, -1) %>% factor()
ytest <- as.factor(phish[test_index, ]$Result)
confusionMatrix(y_hat, ytest)
#Accuracy on test set:
test_accuracy_glm<-confusionMatrix(y_hat, ytest)$overall["Accuracy"]
#F1 Score on test set:
test_f1score_glm<-F1_Score(y_hat, ytest)
#Accuracy on train set:
p_hat <- predict(glm_fit, train_set, type = "response")
d<-0.5
y_hat <- ifelse(p_hat >= d, 1, -1) %>% factor()
ytrain<-as.factor(phish[-test_index, ]$Result)
train_accuracy_glm<-confusionMatrix(y_hat, ytrain)$overall["Accuracy"]
#Creating a vector of predictions:
predictions_glm<-c(train_accuracy_glm, test_accuracy_glm, test_f1score_glm)
predictions_glm
set.seed(820, sample.kind = "Rounding")
y<-phish$Result
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
test_set <- phish[test_index, ]
train_set <- phish[-test_index, ]
set.seed(820, sample.kind = "Rounding")
train_knn <- train(Result ~ ., method = "knn",
data = train_set)
train_knn <- train(Result ~ ., method = "knn",
data = train_set,
tuneGrid = data.frame(k = seq(3, 21, 2)))
#Making plot of k vs accuracy on train set:
ggplot(train_knn) +
theme(plot.title = element_text(hjust = 0.5)) +
labs(title="Accuracy vs k",
x="k",
y="Accuracy")
train_knn$bestTune
set.seed(820, sample.kind = "Rounding")
y_hat <- predict(train_knn, test_set, type = "raw")
ytest <- as.factor(test_set$Result)
confusionMatrix(y_hat, ytest)
#Accuracy on test set:
test_accuracy_knn<-confusionMatrix(y_hat, ytest)$overall["Accuracy"]
#F1 Score on test set
test_f1score_knn<-F1_Score(y_hat, ytest)
#Accuracy on train set:
set.seed(820, sample.kind = "Rounding")
y_hat<-predict(train_knn, train_set, type = "raw")
ytrain<-as.factor(train_set$Result)
train_accuracy_knn<-confusionMatrix(y_hat, ytrain)$overall["Accuracy"]
#Creating a vector of predictions:
predictions_knn<-c(train_accuracy_knn, test_accuracy_knn, test_f1score_knn)
predictions_knn
set.seed(820, sample.kind = "Rounding")
y<-phish$Result
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
test_set <- phish[test_index, ]
train_set <- phish[-test_index, ]
set.seed(820, sample.kind = "Rounding")
train_svm <- train(Result ~ ., method = "svmLinearWeights2",
data = train_set)
set.seed(820, sample.kind = "Rounding")
y_hat <- predict(train_svm, test_set, type = "raw")
ytest <- as.factor(test_set$Result)
confusionMatrix(y_hat, ytest)
#Accuracy on test set:
test_accuracy_svm<-confusionMatrix(y_hat, ytest)$overall["Accuracy"]
#F1 Score on test set:
test_f1score_svm<-F1_Score(y_hat, ytest)
#Accuracy on train set:
set.seed(820, sample.kind = "Rounding")
y_hat<-predict(train_svm, train_set, type = "raw")
ytrain<-as.factor(train_set$Result)
train_accuracy_svm<-confusionMatrix(y_hat, ytrain)$overall["Accuracy"]
#Creating a vector of predictions:
predictions_svm<-c(train_accuracy_svm, test_accuracy_svm, test_f1score_svm)
predictions_svm
set.seed(820, sample.kind = "Rounding")
y<-phish$Result
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
test_set <- phish[test_index, ]
train_set <- phish[-test_index, ]
set.seed(820, sample.kind = "Rounding")
fitControl <- trainControl(method = "repeatedcv", number = 4, repeats = 4)
train_gbm <- train(Result ~ ., data = train_set,
method = "gbm", verbose = FALSE, trControl = fitControl)
set.seed(820, sample.kind = "Rounding")
y_hat <- predict(train_gbm, test_set, type = "raw")
ytest <- as.factor(test_set$Result)
confusionMatrix(y_hat, ytest)
#Accuracy on test set:
test_accuracy_gbm<-confusionMatrix(y_hat, ytest)$overall["Accuracy"]
#F1 Score on test set:
test_f1score_gbm<-F1_Score(y_hat, ytest)
#Accuracy on train set:
set.seed(820, sample.kind = "Rounding")
y_hat<-predict(train_gbm, train_set, type = "raw")
ytrain<-as.factor(train_set$Result)
train_accuracy_gbm<-confusionMatrix(y_hat, ytrain)$overall["Accuracy"]
#Creating a vector of predictions:
predictions_gbm<-c(train_accuracy_gbm, test_accuracy_gbm, test_f1score_gbm)
predictions_gbm
set.seed(820, sample.kind = "Rounding")
y<-phish$Result
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
test_set <- phish[test_index, ]
train_set <- phish[-test_index, ]
set.seed(820, sample.kind = "Rounding")
dir.create(file.path(getwd(), "figures"))
png("figures/decision-tree.png", res=144, pointsize = 0.5,  height=1500, width=4000)
output.tree<-ctree(Result~., data=train_set)
plot(as.simpleparty(output.tree),
tp_args = list(FUN = function(info)
format(info$prediction, nsmall = 1)))
dev.off()
#Note: Here, a rather cumbersome method of plotting has been used. This is because of the large size of the decision tree. The png() function allows one to manipulate the resolution, point size and various other parameters that have been exploited for enhancing the representation of the decision tree. The usage on png() reguires the plot to be saved as an image. Hence a sub directory is created to store the plot, which is then loaded as shown below:
knitr::include_graphics("figures/decision-tree.png")
unlink("figures")
set.seed(820, sample.kind = "Rounding")
y_hat <- predict(output.tree, test_set)
ytest <- as.factor(test_set$Result)
confusionMatrix(y_hat, ytest)
#Accuracy on test set:
test_accuracy_decisiontree<-confusionMatrix(y_hat, ytest)$overall["Accuracy"]
#F1 Score on test set:
test_f1score_decisiontree<-F1_Score(y_hat, ytest)
#Accuracy on train set:
set.seed(820, sample.kind = "Rounding")
y_hat<-predict(output.tree, train_set)
ytrain<-as.factor(train_set$Result)
train_accuracy_decisiontree<-confusionMatrix(y_hat, ytrain)$overall["Accuracy"]
#Creating a vector of predictions:
predictions_decisiontree<-c(train_accuracy_decisiontree, test_accuracy_decisiontree, test_f1score_decisiontree)
predictions_decisiontree
set.seed(820, sample.kind = "Rounding")
y<-phish$Result
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
test_set <- phish[test_index, ]
train_set <- phish[-test_index, ]
set.seed(820, sample.kind = "Rounding")
set.seed(820, sample.kind = "Rounding")
y_hat <- predict(output.tree, test_set)
ytest <- as.factor(test_set$Result)
confusionMatrix(y_hat, ytest)
#Accuracy on test set:
test_accuracy_decisiontree<-confusionMatrix(y_hat, ytest)$overall["Accuracy"]
#F1 Score on test set:
test_f1score_decisiontree<-F1_Score(y_hat, ytest)
#Accuracy on train set:
set.seed(820, sample.kind = "Rounding")
y_hat<-predict(output.tree, train_set)
ytrain<-as.factor(train_set$Result)
train_accuracy_decisiontree<-confusionMatrix(y_hat, ytrain)$overall["Accuracy"]
#Creating a vector of predictions:
predictions_decisiontree<-c(train_accuracy_decisiontree, test_accuracy_decisiontree, test_f1score_decisiontree)
predictions_decisiontree
set.seed(820, sample.kind = "Rounding")
y<-phish$Result
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
test_set <- phish[test_index, ]
train_set <- phish[-test_index, ]
set.seed(820, sample.kind = "Rounding")
train_rf <- train(Result ~ .,
method = "Rborist",
tuneGrid = data.frame(predFixed = 2, minNode = c(3, 50)),
data = train_set)
#Plotting accuracy vs minimum node size:
ggplot(train_rf)  +
theme(plot.title = element_text(hjust = 0.5, size=12),
axis.text.x = element_text(size=9),
axis.text.y = element_text(size=9)) +
labs(title="Accuracy vs Minimal Node Size")
set.seed(820, sample.kind = "Rounding")
y_hat <- predict(train_rf, test_set, type = "raw")
ytest <- as.factor(test_set$Result)
confusionMatrix(y_hat, ytest)
#Accuracy on test set:
test_accuracy_rf<-confusionMatrix(y_hat, ytest)$overall["Accuracy"]
#F1 Score set:
test_f1score_rf<-F1_Score(y_hat, ytest)
#Accuracy on train set:
set.seed(820, sample.kind = "Rounding")
y_hat<-predict(train_rf, train_set, type = "raw")
ytrain<-as.factor(train_set$Result)
train_accuracy_rf<-confusionMatrix(y_hat, ytrain)$overall["Accuracy"]
#Creating a vector of predictions:
predictions_rf<-c(train_accuracy_rf, test_accuracy_rf, test_f1score_rf)
predictions_rf
#Plotting the variable importance:
ggplot(varImp(train_rf)) +
theme(plot.title = element_text(hjust = 0.5)) +
labs(title="Variable Importance")
#Making a data frame for accuracy on train set for each of the algorithms by utilixzing the vectors of predictions:
values<-c("train_accuracy", "test_accuracy", "test_f1score")
algorithm_predictions<-data.frame(lm=predictions_lm,
glm=predictions_glm,
knn=predictions_knn,
svm=predictions_svm,
gbm=predictions_gbm,
decisiontree=predictions_decisiontree,
rf=predictions_rf)
algorithm_predictions<-as.data.frame(t(as.matrix(algorithm_predictions)))
colnames(algorithm_predictions)<-values
algorithm<-c("Linear Regression", "Logistic Regression", "k-nearest neigbours", "Support Vector Machines", "Gradient Boosting Machines", "Decision Tree", "Random Forest")
algorithm_predictions<-cbind(algorithm, algorithm_predictions)
rownames(algorithm_predictions) <- NULL
#Making a lollipop plot for the same:
algorithm_predictions %>% mutate(algorithm=reorder(algorithm,train_accuracy)) %>%
ggplot(aes(x=algorithm, y=train_accuracy, label=(round(train_accuracy, 4)) )) +
geom_segment(aes(x=algorithm, xend=algorithm, y=0, yend=train_accuracy)) +
geom_point() +
geom_point(size=5, color="red", fill=alpha("orange", 0.3), alpha=0.7) +
geom_text( position = position_nudge(y = 0.1), size=3.5) +
coord_flip() +
theme(plot.title = element_text(hjust = 0.5),
axis.text.x = element_text(size=9),
axis.text.y = element_text(size=9)) +
labs(title="Accuracy on Train Set by Different Algorithms",
x="Algorithm",
y="Accuracy on Train Set")
set.seed(820, sample.kind = "Rounding")
y <- phish$Result
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
test_set <- phish[test_index, ]
train_set <- phish[-test_index, ]
set.seed(820, sample.kind = "Rounding")
#Generating the models:
models <- c("rf", "gbm", "knn", "svmLinearWeights2")
set.seed(820, sample.kind = "Rounding")
fits <- lapply(models, function(model){
print(model)
train(Result ~ ., method = model, data = train_set)})
#Generating the models:
models <- c("rf", "gbm", "knn", "svmLinearWeights2")
set.seed(820, sample.kind = "Rounding")
fits <- lapply(models, function(model){
print(model)
train(Result ~ ., method = model, data = train_set)})
#Applying the models on the test set and creating a data frame:
set.seed(820, sample.kind = "Rounding")
mod <- sapply(fits, function(fit){
predict(fit, test_set)})
mod <- as.data.frame(mod, stringsAsFactors =TRUE)
colnames(mod) <- models
library(kableExtra)
kbl(head(mod), longtable=T, booktabs=T, caption="First 6 Rows of the Data Set Mod") %>%
column_spec(1:3, width="2cm") %>%
column_spec(4, width="3.5cm") %>%
kable_styling(latex_options=c("repeat_header"))
